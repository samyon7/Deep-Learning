inputs = tf.keras.layers.Input(shape=(IMG_SIZE,IMG_SIZE,3)) # Must 150 by 150
x = tf.keras.layers.Conv2D(8, (2,2), activation='relu',name="1")(inputs)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Conv2D(8, (2,2), activation='relu',name="2")(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Conv2D(8, (2,2), activation='relu',name="3")(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Conv2D(16, (2,2), activation='relu',name="4")(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Conv2D(16, (2,2), activation='relu',name="5")(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Conv2D(16, (2,2), activation='relu',name="6")(x)
x = tf.keras.layers.BatchNormalization(name="start_branch")(x)

# Branch 1
x1 = tf.keras.layers.Conv2D(16, (2,2), activation='relu',name="1_1")(x)
x1 = tf.keras.layers.Activation("relu")(x1)
x1 = tf.keras.layers.Conv2D(16, (2,2), activation='relu',name="1_2")(x1)
x1 = tf.keras.layers.Activation("relu")(x1)
x1 = tf.keras.layers.Conv2D(32, (2,2), activation='relu',name="1_3")(x1)
x1 = tf.keras.layers.Activation("relu")(x1)
x1 = tf.keras.layers.Conv2D(32, (2,2), activation='relu',name="1_4")(x1)
x1 = tf.keras.layers.Activation("relu")(x1)
x1 = tf.keras.layers.Conv2D(16, (2,2), activation='relu',name="1_5")(x1)
x1 = tf.keras.layers.BatchNormalization()(x1)
x1 = tf.keras.layers.Activation("relu")(x1)

# Branch 2
x2 = tf.keras.layers.Conv2D(16, (6,6), activation='relu', name = "Branch_2")(x)
x2 = tf.keras.layers.BatchNormalization()(x2)
x2 = tf.keras.layers.Activation("relu")(x2)

sum_layer_1 = tf.keras.layers.Concatenate(name="concat")([x1,x2])

x = tf.keras.layers.Conv2D(32, (2,2), activation='relu',name="2_1")(sum_layer_1)
x = tf.keras.layers.Conv2D(32, (2,2), activation='relu')(x)
x = tf.keras.layers.Activation("selu")(x)
x = tf.keras.layers.Conv2D(32, (2,2), activation='relu')(x)
x = tf.keras.layers.Activation("selu")(x)
x = tf.keras.layers.Conv2D(32, (2,2), activation='relu')(x)
x = tf.keras.layers.Activation("selu")(x)
x = tf.keras.layers.Conv2D(32, (2,2), activation='relu')(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Activation("selu", name="Start_Second_Branch")(x)

# Rebranch, there will be 3 branches

x1 = tf.keras.layers.Conv2D(32, (2,2), activation='relu')(x)
x1 = tf.keras.layers.MaxPooling2D(2, 2)(x1)
x1 = tf.keras.layers.BatchNormalization()(x1)
x1 = tf.keras.layers.Conv2D(32, (2,2), activation='relu')(x1)
x1 = tf.keras.layers.MaxPooling2D(2, 2)(x1)
x1 = tf.keras.layers.BatchNormalization()(x1)
x1 = tf.keras.layers.MaxPooling2D(2, 2)(x1)
x1 = tf.keras.layers.Conv2D(64, (2,2), activation='relu')(x1)
x1 = tf.keras.layers.Conv2D(64, (10,10), activation='relu')(x1)
x1 = tf.keras.layers.MaxPooling2D(2, 2)(x1)
x1 = tf.keras.layers.BatchNormalization()(x1)
x1 = tf.keras.layers.Conv2D(64, (2,2), activation='relu')(x1)
x1 = tf.keras.layers.BatchNormalization()(x1)
x1 = tf.keras.layers.Activation("elu")(x1)

x2 = tf.keras.layers.Conv2D(32, (2,2))(x)
x2 = tf.keras.layers.Activation("selu")(x2)
x2 = tf.keras.layers.MaxPooling2D(2, 2)(x2)
x2 = tf.keras.layers.Conv2D(32, (2,2))(x2)
x2 = tf.keras.layers.Activation("selu")(x2)
x2 = tf.keras.layers.MaxPooling2D(2, 2)(x2)
x2 = tf.keras.layers.Conv2D(64, (2,2))(x2)
x2 = tf.keras.layers.Activation("relu")(x2)
x2 = tf.keras.layers.MaxPooling2D(3, 3)(x2)
x2 = tf.keras.layers.Conv2D(64, (2,2))(x2)
x2 = tf.keras.layers.BatchNormalization()(x2)
x2 = tf.keras.layers.MaxPooling2D(2, 2)(x2)
x2 = tf.keras.layers.Activation("relu")(x2)

x3 = tf.keras.layers.Conv2D(32, (2,2), activation='relu')(x)
x3 = tf.keras.layers.MaxPooling2D(4, 4)(x3)
x3 = tf.keras.layers.Conv2D(32, (2,2), activation='relu')(x3)
x3 = tf.keras.layers.MaxPooling2D(2, 2)(x3)
x3 = tf.keras.layers.BatchNormalization()(x3)
x3 = tf.keras.layers.Conv2D(32, (2,2), activation='relu')(x3)
x3 = tf.keras.layers.MaxPooling2D(2, 2)(x3)
x3 = tf.keras.layers.Conv2D(32, (2,2), activation='relu')(x3)
x3 = tf.keras.layers.BatchNormalization()(x3)
x3 = tf.keras.layers.Conv2D(64, (5,5), activation='relu')(x3)
x3 = tf.keras.layers.BatchNormalization()(x3)
x3 = tf.keras.layers.Activation("relu")(x3)


sum_layer_1 = tf.keras.layers.Concatenate(name="reconcat")([x1,x2,x3])
sum_layer_1 = tf.keras.layers.BatchNormalization()(sum_layer_1)
sum_layer_1 = tf.keras.layers.BatchNormalization()(sum_layer_1)
sum_layer_1 = tf.keras.layers.BatchNormalization()(sum_layer_1)
sum_layer_1 = tf.keras.layers.BatchNormalization()(sum_layer_1)
sum_layer_1 = tf.keras.layers.BatchNormalization()(sum_layer_1)
sum_layer_1 = tf.keras.layers.BatchNormalization()(sum_layer_1)
sum_layer_1 = tf.keras.layers.BatchNormalization()(sum_layer_1)
# Reconstruct final architecture!


x3 = tf.keras.layers.Flatten()(sum_layer_1)
x3 = x3 = tf.keras.layers.Dropout(0.5)(x3)
x3 = tf.keras.layers.Dense(512, activation='relu')(x3)
outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x3)
model = tf.keras.Model(inputs=inputs, outputs=outputs)